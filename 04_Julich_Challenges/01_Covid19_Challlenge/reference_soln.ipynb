{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# The COVIDNetX challenge"
   ],
   "metadata": {
    "id": "0yAIJhY1M41M"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"https://www.psycharchives.org/retrieve/096175aa-f7f2-4970-989d-d934c30b5551\" alt=\"drawing\" width=\"400\"/>"
   ],
   "metadata": {
    "id": "NQyQqzQTM41O"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following is a classification challenge using the COVID-X dataset (https://github.com/lindawangg/COVID-Net/blob/master/docs/COVIDx.md).\n",
    "The goal is to predict whether a person has COVID-19 or not based on chest X-RAY images.\n",
    "\n",
    "There are two different categories: `positive` and `negative`.\n",
    "`positive` means a person has COVID-19, `negative` means a person\n",
    "has not COVID-19.\n",
    "\n",
    "The metric we use is F1 (https://en.wikipedia.org/wiki/F1_score). The goal\n",
    "is to maximize F1.\n",
    "\n",
    "The data contains images with their associated labels."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Exploratory Data Analysis"
   ],
   "metadata": {
    "id": "QIeCjvyTM418"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "from IPython.display import Image\n",
    "\n",
    "data_dir = 'data/'\n",
    "# data_dir = 'data_subset/'"
   ],
   "outputs": [],
   "metadata": {
    "id": "V6gETkD5M418"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, you can open the submision.csv file (File -> Open) file and download it!\n",
    "\n",
    "After you download it, you can upload it to the challenge frontend."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Baseline simple solution (Logistic Regression on the top of Resnet50 features)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this simple baseline solution, we use the learned representation by a Resnet-50 as features\n",
    "and use the logistic regression as a model (no fine-tuning here)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "import time\n",
    "import os\n",
    "\n",
    "class CustomDataSet(Dataset):\n",
    "    def __init__(self, filenames, transform=None, labels=None):\n",
    "        self.filenames = filenames\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.filenames[idx]).convert(\"RGB\")\n",
    "        label = self.labels[idx] if self.labels is not None else 0\n",
    "        if self.transform:\n",
    "          tensor_image = self.transform(image)\n",
    "        return tensor_image, label\n",
    "\n",
    "normalize = transforms.Normalize(\n",
    "    mean=[0.485, 0.456, 0.406],\n",
    "    std=[0.229, 0.224, 0.225]\n",
    ")\n",
    "transform =  transforms.Compose([\n",
    "      transforms.Resize(256),\n",
    "      transforms.CenterCrop(224),\n",
    "      transforms.ToTensor(),\n",
    "      normalize\n",
    "])\n",
    "classname_to_index = {\"positive\": 1, \"negative\": 0}\n",
    "index_to_classname = {i:n for n, i in classname_to_index.items()}\n",
    "\n",
    "def build_dataset(df, split):\n",
    "  filenames = [os.path.join(split, name) for name in df.image]\n",
    "  labels = [classname_to_index[name] for name in df.label]\n",
    "  dataset = CustomDataSet(filenames, transform=transform, labels=labels)\n",
    "  return dataset\n",
    "\n",
    "df_train = pd.read_csv(data_dir+'train.csv')\n",
    "df_test = pd.read_csv(data_dir+'submission_valid.csv')\n",
    "train_dataset = build_dataset(df_train.sample(frac=0.1), data_dir+\"train\")\n",
    "test_dataset = build_dataset(df_test.sample(frac=0.1), data_dir+\"valid\")\n",
    "len(train_dataset), len(test_dataset)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "batch_size = 100\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from tqdm import tqdm\n",
    "model = torchvision.models.resnet50(pretrained=True)\n",
    "\n",
    "def extract_features(model, dataloader):\n",
    "    Flist = []\n",
    "    Ylist = []\n",
    "    for X, Y in tqdm(dataloader):\n",
    "        x = model.conv1(X)\n",
    "        x = model.bn1(x)\n",
    "        x = model.relu(x)\n",
    "        x = model.maxpool(x)\n",
    "\n",
    "        x = model.layer1(x)\n",
    "        x = model.layer2(x)\n",
    "        x = model.layer3(x)\n",
    "        x = model.layer4(x)\n",
    "        x = model.avgpool(x)\n",
    "        x = x.view(x.size(0), x.size(1))\n",
    "        Flist.append(x)\n",
    "        Ylist.append(Y)\n",
    "    F = torch.cat(Flist).detach().numpy()\n",
    "    Y = torch.cat(Ylist).detach().numpy()\n",
    "    return F, Y"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# resnet50 = torchvision.models.resnet50(pretrained=True)\n",
    "# resnet50\n",
    "\n",
    "# vgg16 = torchvision.models.vgg16(pretrained=True)\n",
    "# vgg16"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\n",
    "xtrain, ytrain = extract_features(model, train_loader)\n",
    "# xtest, ytest = extract_features(model, test_loader)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(xtrain.shape, ytrain.shape)\n",
    "# print(xtest.shape, ytest.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "idx = np.random.choice(range(len(xtrain)), int(0.8*len(xtrain)), replace=False)\n",
    "train_idx = np.zeros(len(xtrain), dtype=bool)\n",
    "train_idx[idx] = True\n",
    "test_idx = ~train_idx"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(max_iter=1000,class_weight=\"balanced\")\n",
    "clf.fit(xtrain[train_idx], ytrain[train_idx])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# ypred_test = clf.predict(xtrain[400:])\n",
    "# ypred_test = [index_to_classname[y] for y in ypred_test]\n",
    "# print(ypred_test[0:10])\n",
    "# df = pd.read_csv(data_dir+'submission_valid.csv')\n",
    "# df[\"label\"] = ypred_test\n",
    "# df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "print(\"############################## TRAIN ############################## \")\n",
    "y_pred = clf.predict(xtrain[train_idx])\n",
    "print(classification_report(ytrain[train_idx], y_pred))\n",
    "\n",
    "print(\"############################## TEST ############################## \")\n",
    "y_pred = clf.predict(xtrain[test_idx])\n",
    "print(classification_report(ytrain[test_idx], y_pred))\n",
    "# cm = confusion_matrix(ytrain[test_idx], y_pred)\n",
    "# sns.heatmap(cm, cmap='plasma', annot=True);"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# df.to_csv(\"submission.csv\", index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, you can open the submision.csv file (File -> Open) file and download it!\n",
    "\n",
    "After you download it, you can upload it to the challenge frontend."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Baseline Solution with Resnet-50 Fine-tuning"
   ],
   "metadata": {
    "id": "Zr6DPp06p39h"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Code adapted from https://github.com/pytorch/examples/tree/master/imagenet\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import time\n",
    "import os\n",
    "import shutil\n",
    "# device = \"cpu\"\n",
    "device = \"cuda\"\n",
    "\n",
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "    batch_time = AverageMeter('Time', ':6.3f')\n",
    "    data_time = AverageMeter('Data', ':6.3f')\n",
    "    losses = AverageMeter('Loss', ':.4e')\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "    progress = ProgressMeter(\n",
    "        len(train_loader),\n",
    "        [batch_time, data_time, losses, top1, top5],\n",
    "        prefix=\"Epoch: [{}]\".format(epoch))\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (images, target) in enumerate(train_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        images = images.to(device)\n",
    "        target = target.to(device)\n",
    "        # compute output\n",
    "        output = model(images)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        (acc1,) = accuracy(output, target, topk=(1,))\n",
    "        losses.update(loss.item(), images.size(0))\n",
    "        top1.update(acc1[0], images.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            progress.display(i)\n",
    "\n",
    "\n",
    "def validate(val_loader, model, criterion):\n",
    "    batch_time = AverageMeter('Time', ':6.3f')\n",
    "    losses = AverageMeter('Loss', ':.4e')\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "    progress = ProgressMeter(\n",
    "        len(val_loader),\n",
    "        [batch_time, losses, top1, top5],\n",
    "        prefix='Test: ')\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        end = time.time()\n",
    "        for i, (images, target) in enumerate(val_loader):\n",
    "            images = images.to(device)\n",
    "            target = target.to(device)\n",
    "            # compute output\n",
    "            output = model(images)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            acc1, = accuracy(output, target, topk=(1,))\n",
    "            losses.update(loss.item(), images.size(0))\n",
    "            top1.update(acc1[0], images.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "        # TODO: this should also be done with the ProgressMeter\n",
    "        print(' * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}'\n",
    "              .format(top1=top1, top5=top5))\n",
    "\n",
    "    return top1.avg\n",
    "\n",
    "\n",
    "def save_checkpoint(state, is_best, filename='models/checkpoint.pth.tar'):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, 'models/model_best.pth.tar')\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "\n",
    "\n",
    "class ProgressMeter(object):\n",
    "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
    "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
    "        self.meters = meters\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def display(self, batch):\n",
    "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
    "        entries += [str(meter) for meter in self.meters]\n",
    "        print('\\t'.join(entries))\n",
    "\n",
    "    def _get_batch_fmtstr(self, num_batches):\n",
    "        num_digits = len(str(num_batches // 1))\n",
    "        fmt = '{:' + str(num_digits) + 'd}'\n",
    "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch, base_lr):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
    "    lr = base_lr * (0.1 ** (epoch // 30))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res\n"
   ],
   "outputs": [],
   "metadata": {
    "id": "PsC0uqOGtQPb"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class CustomDataSet(Dataset):\n",
    "    def __init__(self, filenames, transform=None, labels=None):\n",
    "        self.filenames = filenames\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.filenames[idx]).convert(\"RGB\")\n",
    "        label = self.labels[idx] if self.labels is not None else 0\n",
    "        if self.transform:\n",
    "          tensor_image = self.transform(image)\n",
    "        return tensor_image, label\n",
    "\n",
    "normalize = transforms.Normalize(\n",
    "    mean=[0.485, 0.456, 0.406],\n",
    "    std=[0.229, 0.224, 0.225]\n",
    ")\n",
    "transform =  transforms.Compose([\n",
    "      transforms.Resize(256),\n",
    "      transforms.CenterCrop(224),\n",
    "      transforms.ToTensor(),\n",
    "      normalize\n",
    "])\n",
    "classname_to_index = {\"positive\": 1, \"negative\": 0}\n",
    "nb_classes = len(classname_to_index)\n",
    "index_to_classname = {idx:name for name, idx in classname_to_index.items()}\n",
    "def build_dataset(df, split):\n",
    "  filenames = [os.path.join(split, name) for name in df.image]\n",
    "  labels = [classname_to_index[name] for name in df.label]\n",
    "  dataset = CustomDataSet(filenames, transform=transform, labels=labels)\n",
    "  return dataset\n",
    "\n",
    "\n",
    "df_train_full = pd.read_csv(data_dir+'train.csv')\n",
    "df_test = pd.read_csv(data_dir+'submission_valid.csv')\n",
    "nb =int(len(df_train_full)*0.9)\n",
    "df_train = df_train_full.iloc[0:nb]\n",
    "df_valid = df_train_full.iloc[nb:]\n",
    "train_dataset = build_dataset(df_train, data_dir+\"train\")\n",
    "valid_dataset = build_dataset(df_valid, data_dir+\"train\")\n",
    "test_dataset = build_dataset(df_test, data_dir+\"valid\")"
   ],
   "outputs": [],
   "metadata": {
    "id": "5Bu1tNT-p3UZ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "batch_size = 16\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    valid_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False\n",
    ")"
   ],
   "outputs": [],
   "metadata": {
    "id": "dPjDI8tjsfgT"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torchvision\n",
    "import torch.nn as nn\n",
    "model = torchvision.models.resnet50(pretrained=True).to(device)\n",
    "model.fc = nn.Linear(2048, nb_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 0.0001\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)"
   ],
   "outputs": [],
   "metadata": {
    "id": "_taT2bywsu6k"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\n",
    "best_acc1 = 0.0\n",
    "for epoch in range(20):\n",
    "    adjust_learning_rate(optimizer, epoch, base_lr=lr)\n",
    "    train(train_loader, model, criterion, optimizer, epoch)\n",
    "    # evaluate on validation set\n",
    "    acc1 = validate(val_loader, model, criterion)\n",
    "    # remember best acc@1 and save checkpoint\n",
    "    is_best = acc1 > best_acc1\n",
    "    best_acc1 = max(acc1, best_acc1)\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'best_acc1': best_acc1,\n",
    "        'optimizer' : optimizer.state_dict(),\n",
    "    }, is_best)"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 400
    },
    "id": "hWYh23WvsmSn",
    "outputId": "8768e243-47fc-4976-b2cb-d159309fe3a0"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Submission"
   ],
   "metadata": {
    "id": "oN_lwHcSM42f"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\n",
    "ypred_test = []\n",
    "for X, _ in test_loader:\n",
    "    with torch.no_grad():\n",
    "        X = X.to(device)\n",
    "        y = model(X)\n",
    "        y = y.cpu()\n",
    "        _, pred = y.max(dim=1)\n",
    "    pred = pred.tolist()\n",
    "    ypred_test.extend([index_to_classname[p] for p in pred])"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "id": "chzZo_AbvjEC",
    "outputId": "964bec8f-28e0-4ec6-f39f-6c6c87cdd130"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(ypred_test[0:10])\n",
    "df = pd.read_csv('submission_valid.csv')\n",
    "df[\"label\"] = ypred_test\n",
    "df.head()"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "-hWJzsYTM42j",
    "outputId": "9a041915-75c1-406f-fdd7-0e59be803a2d"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.to_csv(\"submission.csv\", index=False)"
   ],
   "outputs": [],
   "metadata": {
    "id": "RGJkD57eM42v"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, you can open the submision.csv file (File -> Open) file and download it!\n",
    "\n",
    "After you download it, you can upload it to the challenge frontend."
   ],
   "metadata": {
    "id": "zKcxwPiyM42x"
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "notebook.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('torchEnv': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "interpreter": {
   "hash": "c3b1f0aa73ce6a53838a2507dee39748d09bb5596497487604e75b434ade7b74"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
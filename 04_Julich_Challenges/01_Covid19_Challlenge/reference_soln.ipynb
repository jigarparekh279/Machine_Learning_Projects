{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0yAIJhY1M41M"
   },
   "source": [
    "# The COVIDNetX challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NQyQqzQTM41O"
   },
   "source": [
    "<img src=\"https://www.psycharchives.org/retrieve/096175aa-f7f2-4970-989d-d934c30b5551\" alt=\"drawing\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a classification challenge using the COVID-X dataset (https://github.com/lindawangg/COVID-Net/blob/master/docs/COVIDx.md).\n",
    "The goal is to predict whether a person has COVID-19 or not based on chest X-RAY images.\n",
    "\n",
    "There are two different categories: `positive` and `negative`.\n",
    "`positive` means a person has COVID-19, `negative` means a person\n",
    "has not COVID-19.\n",
    "\n",
    "The metric we use is F1 (https://en.wikipedia.org/wiki/F1_score). The goal\n",
    "is to maximize F1.\n",
    "\n",
    "The data contains images with their associated labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas torch torchvision scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QIeCjvyTM418"
   },
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V6gETkD5M418"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import Image\n",
    "df_train = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "id": "zqw4yBuvM42C",
    "outputId": "ac5d6b41-66a8-4394-997d-0fb68d8409eb"
   },
   "outputs": [],
   "source": [
    "df_train.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "id": "25XW-me9M42H",
    "outputId": "0ab4f8a8-4de2-447e-8d4a-a98fdbffb7d2"
   },
   "outputs": [],
   "source": [
    "df_train.label.value_counts()/len(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CRAMCIzAM42L",
    "outputId": "ec574220-f8ee-4d20-9358-ebfb304090e3"
   },
   "outputs": [],
   "source": [
    "((df_train.label.value_counts())/len(df_train)).plot(kind='bar',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kLBKJtcfM42P",
    "outputId": "58739d64-5df4-48a4-c3d1-ae1e4a1dc194"
   },
   "outputs": [],
   "source": [
    "df_train[df_train.label=='positive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GT9Z3vxfM42S",
    "outputId": "7c7033c0-0216-4bd8-8eab-bcbf6f5d2288"
   },
   "outputs": [],
   "source": [
    "df_train.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wnlp-xh7M42X"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Q32QsxxM42b",
    "outputId": "1b6f74bb-ec39-4d7c-bf34-7d8edaddb02c"
   },
   "outputs": [],
   "source": [
    "print('COVID-19 example (label=positive)')\n",
    "display(Image('train/5a16409064f78a0f046e9ddcfa10438e.png', width=256, height=256))\n",
    "print('No COVID-19 example (label=negative)')\n",
    "display(Image('train/ea3ce6cbc519bf251f16a8bd69199dfa.png', width=256, height=256))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ffcOqQaepyRE"
   },
   "source": [
    "# Dummy Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After exploring the data, we proceed\n",
    "and do our first submission to the challenge.\n",
    "\n",
    "The following is an example of how to do that\n",
    "for a simple **dummy** model.\n",
    "\n",
    "We first \"train\" the model, here no real training\n",
    "is involved as it is a dummy model (just a constant classifier).\n",
    "\n",
    "Then, we load the submission file: **data/submission_valid.csv**.\n",
    "That file contains the image filenames for the public leaderboard phase.\n",
    "\n",
    "To submit, we need to replace the column \"label\" of the file by the predictions,\n",
    "generate a CSV, then upload that CSV to the challenge website.\n",
    "\n",
    "That is what we will do in the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "df = pd.read_csv('submission_valid.csv')\n",
    "clf = DummyClassifier()\n",
    "clf.fit(df_train.image, df_train.label)\n",
    "ypred = clf.predict(df.image)\n",
    "df[\"label\"] = ypred\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is how the submission file (to be uploaded) look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head submission.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can open the submision.csv file (File -> Open) file and download it!\n",
    "\n",
    "After you download it, you can upload it to the challenge frontend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline simple solution (Logistic Regression on the top of Resnet50 features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this simple baseline solution, we use the learned representation by a Resnet-50 as features\n",
    "and use the logistic regression as a model (no fine-tuning here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "import time\n",
    "import os\n",
    "\n",
    "class CustomDataSet(Dataset):\n",
    "    def __init__(self, filenames, transform=None, labels=None):\n",
    "        self.filenames = filenames\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.filenames[idx]).convert(\"RGB\")\n",
    "        label = self.labels[idx] if self.labels is not None else 0\n",
    "        if self.transform:\n",
    "          tensor_image = self.transform(image)\n",
    "        return tensor_image, label\n",
    "\n",
    "normalize = transforms.Normalize(\n",
    "    mean=[0.485, 0.456, 0.406],\n",
    "    std=[0.229, 0.224, 0.225]\n",
    ")\n",
    "transform =  transforms.Compose([\n",
    "      transforms.Resize(256),\n",
    "      transforms.CenterCrop(224),\n",
    "      transforms.ToTensor(),\n",
    "      normalize\n",
    "])\n",
    "classname_to_index = {\"positive\": 1, \"negative\": 0}\n",
    "index_to_classname = {i:n for n, i in classname_to_index.items()}\n",
    "\n",
    "def build_dataset(df, split):\n",
    "  filenames = [os.path.join(split, name) for name in df.image]\n",
    "  labels = [classname_to_index[name] for name in df.label]\n",
    "  dataset = CustomDataSet(filenames, transform=transform, labels=labels)\n",
    "  return dataset\n",
    "\n",
    "df_train = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('submission_valid.csv')\n",
    "train_dataset = build_dataset(df_train, \"train\")\n",
    "test_dataset = build_dataset(df_test, \"valid\")\n",
    "len(train_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=128, \n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=128, \n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.resnet50(pretrained=True)\n",
    "\n",
    "def extract_features(model, dataloader):\n",
    "    Flist = []\n",
    "    Ylist = []\n",
    "    for X, Y in dataloader:\n",
    "        with torch.no_grad():\n",
    "            x = model.conv1(X)\n",
    "            x = model.bn1(x)\n",
    "            x = model.relu(x)\n",
    "            x = model.maxpool(x)\n",
    "\n",
    "            x = model.layer1(x)\n",
    "            x = model.layer2(x)\n",
    "            x = model.layer3(x)\n",
    "            x = model.layer4(x)\n",
    "            x = model.avgpool(x)\n",
    "            x = x.view(x.size(0), x.size(1))\n",
    "            Flist.append(x)\n",
    "            Ylist.append(Y)\n",
    "    F = torch.cat(Flist).numpy()\n",
    "    Y = torch.cat(Ylist).numpy()\n",
    "    return torch.cat(Flist), torch.cat(Ylist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "xtrain, ytrain = extract_features(model, train_loader)\n",
    "xtest, ytest = extract_features(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xtrain.shape, ytrain.shape)\n",
    "print(xtest.shape, ytest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(max_iter=1000,class_weight=\"balanced\")\n",
    "clf.fit(xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred_test = clf.predict(xtest)\n",
    "ypred_test = [index_to_classname[y] for y in ypred_test]\n",
    "print(ypred_test[0:10])\n",
    "df = pd.read_csv('submission_valid.csv')\n",
    "df[\"label\"] = ypred_test\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can open the submision.csv file (File -> Open) file and download it!\n",
    "\n",
    "After you download it, you can upload it to the challenge frontend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zr6DPp06p39h"
   },
   "source": [
    "# Baseline Solution with Resnet-50 Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PsC0uqOGtQPb"
   },
   "outputs": [],
   "source": [
    "#Code adapted from https://github.com/pytorch/examples/tree/master/imagenet\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import time\n",
    "import os\n",
    "import shutil\n",
    "device = \"cpu\"\n",
    "\n",
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "    batch_time = AverageMeter('Time', ':6.3f')\n",
    "    data_time = AverageMeter('Data', ':6.3f')\n",
    "    losses = AverageMeter('Loss', ':.4e')\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "    progress = ProgressMeter(\n",
    "        len(train_loader),\n",
    "        [batch_time, data_time, losses, top1, top5],\n",
    "        prefix=\"Epoch: [{}]\".format(epoch))\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (images, target) in enumerate(train_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        images = images.to(device)\n",
    "        target = target.to(device)\n",
    "        # compute output\n",
    "        output = model(images)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        (acc1,) = accuracy(output, target, topk=(1,))\n",
    "        losses.update(loss.item(), images.size(0))\n",
    "        top1.update(acc1[0], images.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            progress.display(i)\n",
    "\n",
    "\n",
    "def validate(val_loader, model, criterion):\n",
    "    batch_time = AverageMeter('Time', ':6.3f')\n",
    "    losses = AverageMeter('Loss', ':.4e')\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "    progress = ProgressMeter(\n",
    "        len(val_loader),\n",
    "        [batch_time, losses, top1, top5],\n",
    "        prefix='Test: ')\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        end = time.time()\n",
    "        for i, (images, target) in enumerate(val_loader):\n",
    "            images = images.to(device)\n",
    "            target = target.to(device)\n",
    "            # compute output\n",
    "            output = model(images)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            acc1, = accuracy(output, target, topk=(1,))\n",
    "            losses.update(loss.item(), images.size(0))\n",
    "            top1.update(acc1[0], images.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "        # TODO: this should also be done with the ProgressMeter\n",
    "        print(' * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}'\n",
    "              .format(top1=top1, top5=top5))\n",
    "\n",
    "    return top1.avg\n",
    "\n",
    "\n",
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, 'model_best.pth.tar')\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "\n",
    "\n",
    "class ProgressMeter(object):\n",
    "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
    "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
    "        self.meters = meters\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def display(self, batch):\n",
    "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
    "        entries += [str(meter) for meter in self.meters]\n",
    "        print('\\t'.join(entries))\n",
    "\n",
    "    def _get_batch_fmtstr(self, num_batches):\n",
    "        num_digits = len(str(num_batches // 1))\n",
    "        fmt = '{:' + str(num_digits) + 'd}'\n",
    "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch, base_lr):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
    "    lr = base_lr * (0.1 ** (epoch // 30))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Bu1tNT-p3UZ"
   },
   "outputs": [],
   "source": [
    "class CustomDataSet(Dataset):\n",
    "    def __init__(self, filenames, transform=None, labels=None):\n",
    "        self.filenames = filenames\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.filenames[idx]).convert(\"RGB\")\n",
    "        label = self.labels[idx] if self.labels is not None else 0\n",
    "        if self.transform:\n",
    "          tensor_image = self.transform(image)\n",
    "        return tensor_image, label\n",
    "\n",
    "normalize = transforms.Normalize(\n",
    "    mean=[0.485, 0.456, 0.406],\n",
    "    std=[0.229, 0.224, 0.225]\n",
    ")\n",
    "transform =  transforms.Compose([\n",
    "      transforms.Resize(256),\n",
    "      transforms.CenterCrop(224),\n",
    "      transforms.ToTensor(),\n",
    "      normalize\n",
    "])\n",
    "classname_to_index = {\"positive\": 1, \"negative\": 0}\n",
    "nb_classes = len(classname_to_index)\n",
    "index_to_classname = {idx:name for name, idx in classname_to_index.items()}\n",
    "def build_dataset(df, split):\n",
    "  filenames = [os.path.join(split, name) for name in df.image]\n",
    "  labels = [classname_to_index[name] for name in df.label]\n",
    "  dataset = CustomDataSet(filenames, transform=transform, labels=labels)\n",
    "  return dataset\n",
    "\n",
    "\n",
    "df_train_full = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('submission_valid.csv')\n",
    "nb =int(len(df_train_full)*0.9)\n",
    "df_train = df_train_full.iloc[0:nb]\n",
    "df_valid = df_train_full.iloc[nb:]\n",
    "train_dataset = build_dataset(df_train, \"train\")\n",
    "valid_dataset = build_dataset(df_valid, \"train\")\n",
    "test_dataset = build_dataset(df_test, \"valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dPjDI8tjsfgT"
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=128, \n",
    "    shuffle=True\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    valid_dataset, \n",
    "    batch_size=128, \n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=128, \n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_taT2bywsu6k"
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch.nn as nn\n",
    "model = torchvision.models.resnet50(pretrained=True)\n",
    "model.fc = nn.Linear(2048, nb_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 0.0001\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 400
    },
    "id": "hWYh23WvsmSn",
    "outputId": "8768e243-47fc-4976-b2cb-d159309fe3a0"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "best_acc1 = 0.0\n",
    "for epoch in range(20):\n",
    "    adjust_learning_rate(optimizer, epoch, base_lr=lr)\n",
    "    train(train_loader, model, criterion, optimizer, epoch)\n",
    "    # evaluate on validation set\n",
    "    acc1 = validate(val_loader, model, criterion)\n",
    "    # remember best acc@1 and save checkpoint\n",
    "    is_best = acc1 > best_acc1\n",
    "    best_acc1 = max(acc1, best_acc1)\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'best_acc1': best_acc1,\n",
    "        'optimizer' : optimizer.state_dict(),\n",
    "    }, is_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oN_lwHcSM42f"
   },
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "id": "chzZo_AbvjEC",
    "outputId": "964bec8f-28e0-4ec6-f39f-6c6c87cdd130"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "ypred_test = []\n",
    "for X, _ in test_loader:\n",
    "    with torch.no_grad():\n",
    "        X = X.to(device)\n",
    "        y = model(X)\n",
    "        y = y.cpu()\n",
    "        _, pred = y.max(dim=1)\n",
    "    pred = pred.tolist()\n",
    "    ypred_test.extend([index_to_classname[p] for p in pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "-hWJzsYTM42j",
    "outputId": "9a041915-75c1-406f-fdd7-0e59be803a2d"
   },
   "outputs": [],
   "source": [
    "print(ypred_test[0:10])\n",
    "df = pd.read_csv('submission_valid.csv')\n",
    "df[\"label\"] = ypred_test\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RGJkD57eM42v"
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zKcxwPiyM42x"
   },
   "source": [
    "Now, you can open the submision.csv file (File -> Open) file and download it!\n",
    "\n",
    "After you download it, you can upload it to the challenge frontend."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "notebook.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
